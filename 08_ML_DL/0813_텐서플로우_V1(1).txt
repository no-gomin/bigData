## tensorflow v2.x에서 v1 사용하기

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()





## 독립변수 x가 한개인 경우, linear regression을 구현하기

# data set
x = np.array([1,2,3])
y = np.array([1,2,3])


# eight 와 Bais (처음에는 랜덤값을 셋팅했다가 학습과정에서 변경)
# 랜덤값이 아닌, 임의로 지정해준 값을 넣어도 결과는 동일
W = tf.Variable(tf.random.normal([1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')


# Hypothesis 
H = W*x + b


# cost function(최소 제곱법)
cost = tf.reduce_mean(tf.square(H - y))


# 목적은 cost가 최소가 되는 W와 b를 찾아내는 것
# cost 함수는 제곱의 평균인 2차함수이므로 곡선, 곡선위 미분값이 줄어드는 방향으로 학습 (경사하강법)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)


# Session + variable 노드 (W, b) 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())


# 6000번 학습시키기, (v2에서의 fit함수)
for step in range(1, 6001):
    _, cost_val, W_val, b_val = sess.run([train, cost, W, b])
    if step%300 == 0:
        print("{}번째 cost:{}, W:{}, b:{}".format(step, cost_val, W_val, b_val))


# 최종 회귀식
W_, b_ = sess.run([W, b])
print('H = {}*x + {}'.format(W_[0], b_[0]))    # 최종 W값 * x + 최종 b값






## predict를 하기 위한 placeholder (그래프 실행단계에서 값을 던져줌)

# placeholder 설정
x = tf.placeholder(dtype=tf.float32)
y = tf.placeholder(dtype=tf.float32)


# 던져줄 data set (H = 2x + 3)
x_data = np.array([1, 2, 3])
y_data = np.array([5, 7, 9])


# 학습시킬 때, fee_dict를 통해 지정해둔 placeholder에 던져줌 (그 외는 동일)
for step in range(1, 6001):
    _, cost_val, W_val, b_val = sess.run([train, cost, W, b], feed_dict={x:x_data, y:y_data})
    if step % 200 == 1:
        print("{}번째 cost:{}, W:{}, b:{}".format(step, cost_val, W_val, b_val))


# 예측해 보기
sess.run(H, feed_dict={x:np.array([1,10,20])})   # {x:5} 도 가능






