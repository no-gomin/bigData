## NLTK 패키지

import nltk    
nltk.download()   # 반드시 아래 폴더중에 설치해야함.
	# c:/nltk_data	# d:/nltk_data	# e:/nltk_data
	# c:/Users/컴퓨터이름/nltk_data
	# c:/Users/컴퓨터이름/anaconda3/nltk_data
	# c:/Users/컴퓨터이름/anaconda3/share/nltk_data
	# c:/Users/컴퓨터이름/anaconda3/lib/nltk_data
	# c:/Users/컴퓨터이름/Appdata/Roaming/nltk_data
from nltk.book import *






## 기본 함수 및 클래스

from nltk.tokenize import sent_tokenize
sent_tokenize(text)    # 문장 단위로 쪼갠 list 반환

from nltk.tokenize import word_tokenize
word_tokenize(text)    # 단어 단위로 쪼갠 list 반환

from nltk.tokenize import RegexpTokenizer
ret = RegexpTokenizer('[\w]+')    # 토큰화 할 때 정규표현식을 사용
ret.tokenize(text)






## 형태소(의미가 있는 가장 작은 말의 단위) 분석

자연어 처리의 기본은 형태소 분석과 품사태깅

어간 추출(Stemming)  /  원형 복원(Lemmatizing)  /  품사 태깅(Part of Speech Tagging)

원형복원 : 어간 추출을 할 경우 의미가 달라질 수 있어 원형복원을 함.






## 어간 추출(Stemming)

words = ['sending', 'cooking', 'files', 'lives','crying','dying']

from nltk.stem import PorterStemmer   # 방법1
pst = PorterStemmer()
[pst.stem(w) for w in words]    
# 반환 : ['send', 'cook', 'file', 'live', 'cri', 'die']


from nltk.stem import LancasterStemmer    # 방법2 (가장 많이 사용)
lst = LancasterStemmer()
[lst.stem(w) for w in words]
# 반환 : ['send', 'cook', 'fil', 'liv', 'cry', 'dying']


from nltk.stem import RegexpStemmer    # 방법3
rst = RegexpStemmer('ing')
[rst.stem(w) for w in words]
# 반환 : ['send', 'cook', 'files', 'lives', 'cry', 'dy']






## 원형 복원(Lemmatizing)  /  품사 태깅(Part of Speech Tagging)

from nltk.stem.wordnet import WordNetLemmatizer   # 원형 복원
wl = WordNetLemmatizer()
[ wl.lemmatize(w) for w in words]
# words = ['files', 'lives','crying'] ---> 반환 : ['file', 'life', 'cry']


from nltk.tag import pos_tag    # 품사 태깅
pos_tag(words)
# words = ['this', 'is', 'me'] ---> 반환 : [('this', 'DT'), ('is', 'VBZ'), ('me', 'PRP')]
# https://dbrang.tistory.com/1139 에서 DT, VBZ, PRP의 의미 참고


